              # Multilingual RAG System with REST API and Evaluation
#mount drive
from google.colab import drive
drive.mount('/content/drive')
 

#1.Install required libraries
!pip install langchain faiss-cpu sentence-transformers pypdf fastapi uvicorn openai scikit-learn

!pip install -U langchain-community

#2.Load and Preprocess PDF
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import re

#file path
loader = PyPDFLoader("/content/drive/MyDrive/Colab Notebooks/HSC26-Bangla1st-Paper.pdf")
docs = loader.load()

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\u0980-\u09FFa-zA-Z0-9\s.,?!]', '', text)
    return text.strip()

docs = [clean_text(doc.page_content) for doc in docs]

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = []
for doc in docs:
    chunks.extend(splitter.split_text(doc))
 
#3.Build Vector DB
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

embedder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
embeddings = embedder.encode(chunks, convert_to_numpy=True)

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

faiss.write_index(index, "bangla_book_index.faiss")
np.save("bangla_book_chunks.npy", chunks)

#4.setup API Key
import os
os.environ.pop("OPENAI_API_KEY", None)
from getpass import getpass
from openai import OpenAI

os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

# Quick test
client.models.list()
print("API key is valid.")

#5. RAG System
 index = faiss.read_index("bangla_book_index.faiss")
chunks = np.load("bangla_book_chunks.npy", allow_pickle=True)

def retrieve_relevant_chunks(query, top_k=3):
    q_embedding = embedder.encode([query], convert_to_numpy=True)
    distances, indices = index.search(q_embedding, top_k)
    return [chunks[i] for i in indices[0]], distances[0]

from openai import OpenAI
client = OpenAI()

def generate_answer(query, context):
    prompt = f"""Use the context below to answer the question.
    Context: {context}
    Question: {query}
    Answer in the same language as the question.
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    return response.choices[0].message.content
 
 #6.Test RAG  
query = "অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?"
retrieved, scores = retrieve_relevant_chunks(query)
context = "\n".join(retrieved)
answer = generate_answer(query, context)
print(answer)
 
 #7.REST API 
from fastapi import FastAPI
from pydantic import BaseModel
from sklearn.metrics.pairwise import cosine_similarity

app = FastAPI()

class Query(BaseModel):
    question: str

def groundedness_score(answer, retrieved):
    context = " ".join(retrieved)
    a_emb = embedder.encode([answer])
    c_emb = embedder.encode([context])
    return cosine_similarity(a_emb, c_emb)[0][0]

def relevance_score(query, retrieved):
    q_emb = embedder.encode([query])
    c_emb = embedder.encode([" ".join(retrieved)])
    return cosine_similarity(q_emb, c_emb)[0][0]

@app.post("/ask")
def ask_rag(query: Query):
    retrieved, scores = retrieve_relevant_chunks(query.question)
    context = "\n".join(retrieved)
    answer = generate_answer(query.question, context)
    return {
        "question": query.question,
        "answer": answer,
        "retrieved_chunks": retrieved,
        "scores": scores.tolist(),
        "groundedness": groundedness_score(answer, retrieved),
        "relevance": relevance_score(query.question, retrieved)
    }

#Evaluation 
print("Groundedness:", groundedness_score(answer, retrieved))
print("Relevance:", relevance_score(query, retrieved))
